{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmGPfIwRKzFV"
      },
      "source": [
        "# `Setup`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceRf-vSlKzFW",
        "outputId": "f0388aa0-7ed4-4198-fab2-626246eae361"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import datetime as dt\n",
        "from math import ceil\n",
        "try:\n",
        "    # Mounting Colab Drive if possible\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Cloning repo for colab\n",
        "    if 'aml_itu' in os.getcwd():\n",
        "        %cd aml_itu/\n",
        "        !git pull https://github.com/RasKrebs/aml_itu\n",
        "    else:\n",
        "        !git clone https://github.com/RasKrebs/aml_itu\n",
        "        %cd aml_itu/\n",
        "    os.environ[\"COLAB\"] = \"True\"\n",
        "\n",
        "except:\n",
        "    # Changing directory into aml_itu\n",
        "    if os.getcwd().split('/')[-1] != 'aml_itu': os.chdir(os.path.abspath('.').split('aml_itu/')[0]+'aml_itu')\n",
        "    !git pull origin main --ff-only\n",
        "    os.environ[\"COLAB\"] = \"False\"\n",
        "\n",
        "# Utils Import\n",
        "from utils.helpers import *\n",
        "from utils.StatefarmPytorchDataset import StateFarmDataset\n",
        "\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import v2\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "\n",
        "\n",
        "# Install torchinfo, import if it's available\n",
        "try:\n",
        "  import torchinfo\n",
        "except:\n",
        "  !pip install torchinfo\n",
        "  import torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "\n",
        "# Printing current working directory\n",
        "print(os.getcwd())\n",
        "\n",
        "# Setting up device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print (f\"GPU is available\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print('MPS device found.')\n",
        "else:\n",
        "    print (\"No GPU available, using CPU instead\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qujmd-H5KzFX"
      },
      "source": [
        "### `Config`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR4A88qzKzFX",
        "outputId": "bf171fda-ed9b-42f4-a24a-2a2f83150932"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'EfficientNet'\n",
        "\n",
        "# Loading the config file (if content is in workin directory must mean colab is being used)\n",
        "config = load_config(eval(os.environ[\"COLAB\"]))\n",
        "\n",
        "\n",
        "# Training Images\n",
        "train_img = config['dataset']['images']['train']\n",
        "\n",
        "# Outputting config\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDaRwhJjKzFY"
      },
      "source": [
        "## `TinyVGG`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T9mX5BKKzFY"
      },
      "outputs": [],
      "source": [
        "img_size_L = (168, 224)\n",
        "img_size_M = (93, 124)\n",
        "img_size_S = (48, 64)\n",
        "\n",
        "# IMG Transformations\n",
        "augmentations = {\n",
        "    'train': v2.Compose([\n",
        "    # v2.Grayscale(1),\n",
        "    v2.RandomRotation(degrees=90),\n",
        "    v2.RandomResizedCrop(img_size_M, antialias=True, scale=(0.8, 1)),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.ToDtype(torch.float32, scale=True)]),\n",
        "    'val+test': v2.Compose([\n",
        "    # v2.Grayscale(1),\n",
        "    v2.Resize(img_size_M, antialias=True),\n",
        "    v2.ToDtype(torch.float32, scale=True)])}\n",
        "\n",
        "# Target Transformations (Removing the c from the target)\n",
        "target_transform = T.Lambda(lambda y: torch.tensor(int(y.replace('c', ''))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXckhcO8KzFX"
      },
      "outputs": [],
      "source": [
        "# Normal Dataset\n",
        "# Creating the dataset\n",
        "train_data = StateFarmDataset(config, \n",
        "                              transform=augmentations['train'], \n",
        "                              split='train', \n",
        "                              target_transform=target_transform)\n",
        "\n",
        "print(f'Lenght of train data: {len(train_data)}')\n",
        "\n",
        "# Creating the dataset\n",
        "val_data = StateFarmDataset(config, \n",
        "                            transform=augmentations['val+test'], \n",
        "                            split='val', \n",
        "                            target_transform=target_transform)\n",
        "\n",
        "print(f'Lenght of val data: {len(val_data)}')\n",
        "\n",
        "test_data = StateFarmDataset(config, \n",
        "                            split='test', \n",
        "                            transform=augmentations['val+test'], \n",
        "                            target_transform=target_transform)\n",
        "\n",
        "print(f'Lenght of val data: {len(test_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Displaying training data including transformations\n",
        "train_data.display_classes(id_to_class=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOlK-farQYyk"
      },
      "source": [
        "#### `Model`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUxzvRBVQYyk"
      },
      "outputs": [],
      "source": [
        "''' A simple Convolution, Batch Normalization, and Activation Class'''\n",
        "\n",
        "class ConvBnAct(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_in, n_out, kernel_size = 3, stride = 1, \n",
        "                 padding = 0, groups = 1, bn = True, act = True,\n",
        "                 bias = False\n",
        "                ):\n",
        "        \n",
        "        super(ConvBnAct, self).__init__()\n",
        "        \n",
        "        self.conv = nn.Conv2d(n_in, n_out, kernel_size = kernel_size,\n",
        "                              stride = stride, padding = padding,\n",
        "                              groups = groups, bias = bias\n",
        "                             )\n",
        "        self.batch_norm = nn.BatchNorm2d(n_out) if bn else nn.Identity()\n",
        "        self.activation = nn.SiLU() if act else nn.Identity()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "''' Squeeze and Excitation Block '''\n",
        "\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_in, reduced_dim):\n",
        "        super(SqueezeExcitation, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(n_in, reduced_dim, kernel_size=1),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(reduced_dim, n_in, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "       \n",
        "    def forward(self, x):\n",
        "        \n",
        "        y = self.se(x)\n",
        "        \n",
        "        return x * y\n",
        "                                    \n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "''' Stochastic Depth Module'''\n",
        "\n",
        "class StochasticDepth(nn.Module):\n",
        "    \n",
        "    def __init__(self, survival_prob = 0.8):\n",
        "        super(StochasticDepth, self).__init__()\n",
        "        \n",
        "        self.p =  survival_prob\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        if not self.training:\n",
        "            return x\n",
        "        \n",
        "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.p\n",
        "        \n",
        "        return torch.div(x, self.p) * binary_tensor\n",
        "        \n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "''' Residual Bottleneck Block with Expansion Factor = N as defined in Mobilenet-V2 paper\n",
        "    with Squeeze and Excitation Block and Stochastic Depth. \n",
        "'''\n",
        "\n",
        "class MBConvN(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_in, n_out, kernel_size = 3, \n",
        "                 stride = 1, expansion_factor = 6,\n",
        "                 reduction = 4, # Squeeze and Excitation Block\n",
        "                 survival_prob = 0.8 # Stochastic Depth\n",
        "                ):\n",
        "        \n",
        "        super(MBConvN, self).__init__()\n",
        "        \n",
        "        self.skip_connection = (stride == 1 and n_in == n_out) \n",
        "        intermediate_channels = int(n_in * expansion_factor)\n",
        "        padding = (kernel_size - 1)//2\n",
        "        reduced_dim = int(n_in//reduction)\n",
        "        \n",
        "        self.expand = nn.Identity() if (expansion_factor == 1) else ConvBnAct(n_in, intermediate_channels, kernel_size = 1)\n",
        "        self.depthwise_conv = ConvBnAct(intermediate_channels, intermediate_channels,\n",
        "                                        kernel_size = kernel_size, stride = stride, \n",
        "                                        padding = padding, groups = intermediate_channels\n",
        "                                       )\n",
        "        self.se = SqueezeExcitation(intermediate_channels, reduced_dim = reduced_dim)\n",
        "        self.pointwise_conv = ConvBnAct(intermediate_channels, n_out, \n",
        "                                        kernel_size = 1, act = False\n",
        "                                       )\n",
        "        self.drop_layers = StochasticDepth(survival_prob = survival_prob)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        residual = x\n",
        "        \n",
        "        x = self.expand(x)\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.se(x)\n",
        "        x = self.pointwise_conv(x)\n",
        "        \n",
        "        if self.skip_connection:\n",
        "            x = self.drop_layers(x)\n",
        "            x += residual\n",
        "        \n",
        "        return x\n",
        "    \n",
        "\n",
        "#----------------------------------------------------------------------------------------------\n",
        "\n",
        "'''Efficient-net Class'''\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    \n",
        "    '''Generic Efficient net class which takes width multiplier, Depth multiplier, and Survival Prob.'''\n",
        "    \n",
        "    def __init__(self, width_mult = 1, depth_mult = 1, \n",
        "                 dropout_rate = 0.2, num_classes = 1000):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        \n",
        "        last_channel = ceil(1280 * width_mult)\n",
        "        self.features = self._feature_extractor(width_mult, depth_mult, last_channel)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(last_channel, num_classes)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.classifier(x.view(x.shape[0], -1))\n",
        "        \n",
        "        return x\n",
        "    \n",
        "        \n",
        "    def _feature_extractor(self, width_mult, depth_mult, last_channel):\n",
        "        \n",
        "        channels = 4*ceil(int(32*width_mult) / 4)\n",
        "        layers = [ConvBnAct(3, channels, kernel_size = 3, stride = 2, padding = 1)]\n",
        "        in_channels = channels\n",
        "        \n",
        "        kernels = [3, 3, 5, 3, 5, 5, 3]\n",
        "        expansions = [1, 6, 6, 6, 6, 6, 6]\n",
        "        num_channels = [16, 24, 40, 80, 112, 192, 320]\n",
        "        num_layers = [1, 2, 2, 3, 3, 4, 1]\n",
        "        strides =[1, 2, 2, 2, 1, 2, 1]\n",
        "        \n",
        "        # Scale channels and num_layers according to width and depth multipliers.\n",
        "        scaled_num_channels = [4*ceil(int(c*width_mult) / 4) for c in num_channels]\n",
        "        scaled_num_layers = [int(d * depth_mult) for d in num_layers]\n",
        "\n",
        "        \n",
        "        for i in range(len(scaled_num_channels)):\n",
        "             \n",
        "            layers += [MBConvN(in_channels if repeat==0 else scaled_num_channels[i], \n",
        "                               scaled_num_channels[i],\n",
        "                               kernel_size = kernels[i],\n",
        "                               stride = strides[i] if repeat==0 else 1, \n",
        "                               expansion_factor = expansions[i]\n",
        "                              )\n",
        "                       for repeat in range(scaled_num_layers[i])\n",
        "                      ]\n",
        "            in_channels = scaled_num_channels[i]\n",
        "        \n",
        "        layers.append(ConvBnAct(in_channels, last_channel, kernel_size = 1, stride = 1, padding = 0))\n",
        "    \n",
        "        return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compound scaling factors for efficient-net family.\n",
        "efficient_net_config = {\n",
        "    # tuple of width multiplier, depth multiplier, resolution, and Survival Prob\n",
        "    \"b0\" : (1.1, 1.2, 224, 0.3),\n",
        "    \"b1\" : (1.0, 1.1, 240, 0.2),\n",
        "    \"b2\" : (1.1, 1.2, 260, 0.3),\n",
        "    \"b3\" : (1.2, 1.4, 300, 0.3),\n",
        "    \"b4\" : (1.4, 1.8, 380, 0.4),\n",
        "    \"b5\" : (1.6, 2.2, 456, 0.4),\n",
        "    \"b6\" : (1.8, 2.6, 528, 0.5),\n",
        "    \"b7\" : (2.0, 3.1, 600, 0.5)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvw9GCTCQYyk",
        "outputId": "64201891-f8e7-45e9-978d-84c6bb131e8b"
      },
      "outputs": [],
      "source": [
        "# Initialize Efficientnet model\n",
        "version = 'b0'\n",
        "width_mult, depth_mult, res, dropout_rate = 1., 1., 224, .5 # efficient_net_config[version]\n",
        "#dropout_rate = .5\n",
        "model = EfficientNet(width_mult, depth_mult, dropout_rate, num_classes = 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZbi4fJNQYyk"
      },
      "outputs": [],
      "source": [
        "batch_size = config['modeling_params']['batch_size']\n",
        "epochs = 50 # config['modeling_params']['epochs']\n",
        "seed = 42\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXvq116rQYyk",
        "outputId": "09f8105e-5ec8-404a-dc7d-b27ffe882a7a"
      },
      "outputs": [],
      "source": [
        "# Model summary\n",
        "x, y = next(iter(train_dataloader))\n",
        "\n",
        "summary(model, input_size=x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qijKuDacQYyl"
      },
      "source": [
        "#### `Training Methods`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_training(history, num_epochs=50):\n",
        "    \n",
        "    # Generate Figure\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # Loss Plots\n",
        "    sns.lineplot(y=history['train_loss'], x=list(range(len(history['train_loss']))), ax=axs[0], label='Train Loss')\n",
        "    sns.lineplot(y=history['val_loss'], x=list(range(len(history['val_loss']))), ax=axs[0], label='Validation Loss')\n",
        "    axs[0].set_ylabel('Cross Entropy Loss')\n",
        "    axs[0].set_xlabel('Epochs')\n",
        "    axs[0].set_xlim(0, num_epochs)\n",
        "\n",
        "    # Accuracy Plots\n",
        "    sns.lineplot(y=history['train_acc'], x=list(range(len(history['train_acc']))), ax=axs[1], label='Train Accuracy')\n",
        "    sns.lineplot(y=history['val_acc'], x=list(range(len(history['val_acc']))), ax=axs[1], label='Validation Accuracy')\n",
        "    axs[1].set_ylabel('Accuracy')\n",
        "    axs[1].set_xlabel('Epochs')\n",
        "    axs[1].set_xlim(0, num_epochs)\n",
        "    \n",
        "    # Show plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPo-51GAQYyl"
      },
      "outputs": [],
      "source": [
        "def train_step(model, dataloader, loss_fn,optimizer, device):\n",
        "    \"\"\"Train step for a single epoch. Taken from PyTorch 'Training with PyTorch'\"\"\"\n",
        "    \n",
        "    # Losses and accuracies\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "        \n",
        "        # Extracting data and labels + moving to device\n",
        "        imgs, labels = data\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        \n",
        "        # Zero-ing gradients for every new batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        prediction = model(imgs)\n",
        "        \n",
        "        # Computing Loss and Gradient\n",
        "        loss = loss_fn(prediction, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update train loss and accuracy\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (prediction.argmax(1) == labels).type(torch.float).mean().item()\n",
        "    # Return train loss and accuracy\n",
        "    return train_loss / len(dataloader), train_acc / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P13K9bx-QYyl"
      },
      "outputs": [],
      "source": [
        "def validation(model, dataloader, loss_fn, device):\n",
        "    \"\"\"Validation loop\"\"\"\n",
        "    # Setup validation loss and accuracy\n",
        "    val_loss, val_acc = 0, 0\n",
        "\n",
        "    # Disable gradient calculations\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):   \n",
        "            # Extract imgs and labels and sent to device\n",
        "            imgs, labels = data\n",
        "            imgs, labels  = imgs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass and update validation loss\n",
        "            prediction = model(imgs)\n",
        "            loss = loss_fn(prediction, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Update validation accuracy\n",
        "            val_acc += (prediction.argmax(1) == labels).type(torch.float).mean().item()\n",
        "    # Return validation loss and accuracy\n",
        "    return val_loss / len(dataloader), val_acc / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(model, model_name, epoch):\n",
        "    \"\"\"Function for saving model\"\"\"\n",
        "    # Model name, with path\n",
        "    timestamp = dt.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    file = f'{model_name}_{timestamp}_epoch_{epoch+1}'\n",
        "    name = os.path.join(config['outputs']['path'], model_name, file)\n",
        "    \n",
        "    # Make directory if not exists\n",
        "    if not os.path.exists(os.path.join(os.path.join(config['outputs']['path'], model_name))):\n",
        "        os.makedirs(os.path.join(os.path.join(config['outputs']['path'], model_name)))\n",
        "    \n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), f'{name}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "    \"\"\"Early Stopping Class. Copied from https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\"\"\"\n",
        "    def __init__(self, patience=1, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = float('inf')\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            if self.counter != 0:\n",
        "                print('Early Stopping Counter Reset')\n",
        "            self.counter = 0\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            print(f'Early Stopping Counter {self.counter} of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-ndlrjVQYyl"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataloader, validation_dataloader, optimizer, loss_fn, epochs, device, scheduler, early_stopping, model_name):\n",
        "    \"\"\"Model training method\"\"\"\n",
        "    # History\n",
        "    history = dict(train_loss=[],\n",
        "                   train_acc=[],\n",
        "                   val_loss=[],\n",
        "                   val_acc=[])\n",
        "\n",
        "    # Save start of training\n",
        "    total_training_start = time.time()\n",
        "    # Loop through epochs\n",
        "    for epoch in range(epochs):\n",
        "        print(f'\\nEpoch {epoch+1} of {epochs} started...')\n",
        "        \n",
        "        # Get start training step time\n",
        "        start_train = time.time()\n",
        "        \n",
        "        # Set model to train mode and do pass over data - plus save start of training step\n",
        "        model.train(True)\n",
        "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
        "\n",
        "        # Set model to eval and do pass over validation data + time for start of validation step\n",
        "        model.eval()\n",
        "        val_loss, val_acc = validation(model, validation_dataloader, loss_fn, device)\n",
        "        \n",
        "        # Calculate total time spent\n",
        "        end_train = time.time()\n",
        "        total_m, total_s = divmod(end_train - start_train, 60)\n",
        "        \n",
        "        # lr Scheduler step\n",
        "        if scheduler != None:\n",
        "            scheduler.step(val_loss)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1} of {epochs} ({round(total_m)}:{round(total_s)}) - Train loss: {train_loss:.5f} - Train acc: {train_acc:.5f} - Val loss: {val_loss:.5f} - Val acc: {val_acc:.5f}\")\n",
        "        \n",
        "        \n",
        "        # Save model if val loss is lower than previous lowest\n",
        "        if val_loss < min(history['val_loss'], default=1e10):\n",
        "            print(f\"Saving model with new best val_loss: {val_loss:.5f}\")\n",
        "            \n",
        "            # Save model\n",
        "            save_model(model, model_name, epoch)\n",
        "        \n",
        "        # Save train and val loss/acc\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Visualize every 5th epoch\n",
        "        if (epoch + 1) % 5 == 0: \n",
        "            visualize_training(history, epochs)\n",
        "            \n",
        "        if early_stopping.early_stop(val_loss):             \n",
        "            print(f\"Epoch {epoch+1} of {epochs} - Early stopping\")\n",
        "            print('Saving final model, with loss: ', val_loss)\n",
        "            save_model(model, model_name, epoch)\n",
        "            visualize_training(history, epoch+1)\n",
        "            break\n",
        "    total_training_end = time.time()\n",
        "    minutes, seconds = divmod(total_training_end-total_training_start, 60)\n",
        "    \n",
        "    print(f'Total training time: {round(minutes)}:{round(seconds)}')\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "FVasakcHQYyl",
        "outputId": "4b278096-1a4d-4782-f999-cb1dbe2513a7"
      },
      "outputs": [],
      "source": [
        "# Set random seeds\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "epochs = 50\n",
        "learming_rate = 0.001\n",
        "momentum = .90\n",
        "weight_decay = 1e-4\n",
        "MODEL_NAME = 'EfficientNet'\n",
        "nestrov = False\n",
        "\n",
        "# Initialize Efficientnet model\n",
        "version = 'b0'\n",
        "width_mult, depth_mult, res, dropout_rate = efficient_net_config[version]\n",
        "model = EfficientNet(width_mult, depth_mult, dropout_rate, num_classes = 10)\n",
        "model = model.to(device) # Load model to device.\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learming_rate, weight_decay=weight_decay, momentum=momentum, nesterov=nestrov) # optimizer = torch.optim.RMSprop(model.parameters(), lr=learming_rate, weight_decay=weight_decay, momentum=momentum)\n",
        "\n",
        "# Setup scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                       'min',\n",
        "                                                       factor=.1,\n",
        "                                                       patience=5,\n",
        "                                                       min_lr=1e-7,\n",
        "                                                       verbose=True)\n",
        "\n",
        "# Setup early stopping with a minimum delta of 1.5 - which means if val loss - training loss > min delta, add to counter\n",
        "early_stopping = EarlyStopper(patience=5, min_delta=.03)\n",
        "\n",
        "# results\n",
        "results = train(model=model,\n",
        "                train_dataloader=train_dataloader,\n",
        "                validation_dataloader=val_dataloader,\n",
        "                optimizer=optimizer,\n",
        "                loss_fn=loss_fn,\n",
        "                epochs=epochs,\n",
        "                device=device,\n",
        "                scheduler=scheduler,\n",
        "                early_stopping=early_stopping,\n",
        "                model_name=MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing on Test Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import read_image\n",
        "\n",
        "test_imgs = os.listdir(config['dataset']['images']['test'])\n",
        "test_img = test_imgs[random.randint(0, len(test_imgs))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading image\n",
        "test_image = read_image(os.path.join(config['dataset']['images']['test'], test_img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_image = train_data.transform(test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(test_image.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_pred = model(test_image.unsqueeze(0).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config['dataset']['class_mapping']['c' + str(test_pred.argmax(1).item())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
