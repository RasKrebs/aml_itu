{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmGPfIwRKzFV"
      },
      "source": [
        "# `Setup`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceRf-vSlKzFW",
        "outputId": "cbba2297-3b61-4901-969c-5a663fd2f7e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "fatal: destination path 'aml_itu' already exists and is not an empty directory.\n",
            "/content/aml_itu\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import datetime as dt\n",
        "from math import ceil\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "try:\n",
        "    # Mounting Colab Drive if possible\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Cloning repo for colab\n",
        "    if 'aml_itu' in os.getcwd():\n",
        "        %cd aml_itu/\n",
        "        !git pull https://github.com/RasKrebs/aml_itu\n",
        "        !git checkout -b sparse_filtering\n",
        "    else:\n",
        "        !git clone -b sparse_test https://github.com/RasKrebs/aml_itu\n",
        "        %cd aml_itu/\n",
        "    os.environ[\"COLAB\"] = \"True\"\n",
        "\n",
        "except:\n",
        "    # Changing directory into aml_itu\n",
        "    if os.getcwd().split('/')[-1] != 'aml_itu': os.chdir(os.path.abspath('.').split('aml_itu/')[0]+'aml_itu')\n",
        "    !git pull origin main --ff-only\n",
        "    os.environ[\"COLAB\"] = \"False\"\n",
        "\n",
        "# Utils Import\n",
        "from utils.helpers import *\n",
        "from utils.StatefarmPytorchDataset import StateFarmDataset\n",
        "\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "torchvision.disable_beta_transforms_warning()\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import v2\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Install torchinfo, import if it's available\n",
        "try:\n",
        "  import torchinfo\n",
        "except:\n",
        "  !pip install torchinfo\n",
        "  import torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "\n",
        "# Printing current working directory\n",
        "print(os.getcwd())\n",
        "\n",
        "# Setting up device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print (f\"GPU is available\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print('MPS device found.')\n",
        "else:\n",
        "    print (\"No GPU available, using CPU instead\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qujmd-H5KzFX"
      },
      "source": [
        "### `Config`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR4A88qzKzFX"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'TinyVGG_best'\n",
        "\n",
        "# Loading the config file (if content is in workin directory must mean colab is being used)\n",
        "config = load_config(eval(os.environ[\"COLAB\"]))\n",
        "\n",
        "\n",
        "# Training Images\n",
        "train_img = config['dataset']['images']['train']\n",
        "\n",
        "# Outputting config\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3ABOz6Ol0Zl"
      },
      "outputs": [],
      "source": [
        "def save_model(model, model_name, epoch):\n",
        "    \"\"\"Function for saving model\"\"\"\n",
        "    # Model name, with path\n",
        "    timestamp = dt.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    file = f'{model_name}_{timestamp}_epoch_{epoch+1}'\n",
        "    name = os.path.join(config['outputs']['path'], model_name, file)\n",
        "\n",
        "    # Make directory if not exists\n",
        "    if not os.path.exists(os.path.join(os.path.join(config['outputs']['path'], model_name))):\n",
        "        os.makedirs(os.path.join(os.path.join(config['outputs']['path'], model_name)))\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), f'{name}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RKyqh-awZCv"
      },
      "source": [
        "## Sparse Filtring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2prAFACAwZCv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Sparse Filter Class\n",
        "class SparseFilter(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SparseFilter, self).__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(input_dim, output_dim) * 0.5)\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "    def soft_abs(self, value):\n",
        "        return torch.sqrt(value ** 2 + self.epsilon)\n",
        "\n",
        "    def forward(self, x):\n",
        "        first = torch.matmul(x, self.weights)\n",
        "        second = self.soft_abs(first)\n",
        "        third = second / torch.sqrt(torch.sum(second ** 2, axis=0) + self.epsilon)\n",
        "        fourth = third / torch.sqrt(torch.sum(third ** 2, axis=1)[:, None] + self.epsilon)\n",
        "        return torch.sum(fourth)\n",
        "\n",
        "\n",
        "pretransform = v2.Compose([\n",
        "      v2.ToPILImage(),\n",
        "      v2.Resize((126, 168), antialias=True),\n",
        "      v2.Grayscale(num_output_channels=1),\n",
        "      v2.ToTensor(),\n",
        "      v2.Normalize(mean=[0.485], std=[0.229]),\n",
        "      v2.Lambda(lambda x: torch.flatten(x)),\n",
        "      v2.ToTensor()\n",
        "    ])\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from torch.utils.data import Subset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "\n",
        "# Load the dataset without transformations\n",
        "train_data = StateFarmDataset(config,\n",
        "                              transform=pretransform,  # No image transformations\n",
        "                              split='train',\n",
        "                              target_transform=None)\n",
        "\n",
        "# Generate random indices for the subset\n",
        "subset_size = 10000\n",
        "indices = torch.randperm(len(train_data)).tolist()\n",
        "subset_indices = indices[:subset_size]\n",
        "\n",
        "# Create a subset\n",
        "train_subset = Subset(train_data, subset_indices)\n",
        "\n",
        "# Create a DataLoader for the subset\n",
        "train_subset_loader = DataLoader(train_data, batch_size=32, num_workers=4, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvX-JJI9jCpr"
      },
      "outputs": [],
      "source": [
        "def train_step(model, dataloader, optimizer, device, accumulation_steps=4):\n",
        "    \"\"\"Train step for a single epoch with gradient accumulation.\"\"\"\n",
        "\n",
        "    # Losses and accuracies\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    # Initialize the gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "\n",
        "        # Extracting data and labels + moving to device\n",
        "        imgs, labels = data\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        loss = model(imgs)\n",
        "\n",
        "        # Normalize the loss to account for accumulation\n",
        "        normalized_loss = loss / accumulation_steps\n",
        "\n",
        "        # Backward pass (accumulates gradients over multiple backward steps)\n",
        "        normalized_loss.backward()\n",
        "\n",
        "        # Step with optimizer every 'accumulation_steps' iterations\n",
        "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Update train loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Return average train loss\n",
        "    return train_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e9GmdtYjR_i"
      },
      "outputs": [],
      "source": [
        "def train_sparse_filter(model, train_dataloader, optimizer, epochs, device):\n",
        "    \"\"\"Model training method\"\"\"\n",
        "    # History\n",
        "    history = dict(train_loss=[],\n",
        "                   train_acc=[],\n",
        "                   val_loss=[],\n",
        "                   val_acc=[])\n",
        "\n",
        "    # Loop through epochs\n",
        "    for epoch in range(epochs):\n",
        "        print(f'\\nEpoch {epoch+1} of {epochs} started...')\n",
        "\n",
        "        # Set model to train mode and do pass over data\n",
        "        model.train(True)\n",
        "        train_loss = train_step(model, train_dataloader, optimizer, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} of {epochs} - Train loss: {train_loss:.5f}\")\n",
        "\n",
        "        # Save model\n",
        "        save_model(model, MODEL_NAME, epoch)\n",
        "\n",
        "        # Save train and val loss/acc\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        # Visualize every 5th epoch\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            visualize_training(history, epochs)\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f66bzvW9UT8_"
      },
      "outputs": [],
      "source": [
        "def visualize_training(history, num_epochs=50):\n",
        "\n",
        "    # Generate Figure\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Loss Plots\n",
        "    sns.lineplot(y=history['train_loss'], x=list(range(len(history['train_loss']))), ax=axs[0], label='Train Loss')\n",
        "    sns.lineplot(y=history['val_loss'], x=list(range(len(history['val_loss']))), ax=axs[0], label='Validation Loss')\n",
        "    axs[0].set_ylabel('Cross Entropy Loss')\n",
        "    axs[0].set_xlabel('Epochs')\n",
        "    axs[0].set_xlim(0, num_epochs)\n",
        "\n",
        "    # Accuracy Plots\n",
        "    sns.lineplot(y=history['train_acc'], x=list(range(len(history['train_acc']))), ax=axs[1], label='Train Accuracy')\n",
        "    sns.lineplot(y=history['val_acc'], x=list(range(len(history['val_acc']))), ax=axs[1], label='Validation Accuracy')\n",
        "    axs[1].set_ylabel('Accuracy')\n",
        "    axs[1].set_xlabel('Epochs')\n",
        "    axs[1].set_xlim(0, num_epochs)\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od6JKwpTjrVc"
      },
      "outputs": [],
      "source": [
        "# Define and train the sparse filter model\n",
        "input_dim = 126*168\n",
        "output_dim = 126*168  # Desired output dimension\n",
        "sparse_filter_model = SparseFilter(input_dim, output_dim).to(device)  # Move model to GPU\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.Adam(sparse_filter_model.parameters(), lr=learning_rate)\n",
        "epochs = 50\n",
        "\n",
        "\n",
        "\n",
        "results = train_sparse_filter(model=sparse_filter_model,\n",
        "                train_dataloader=train_subset_loader,\n",
        "                optimizer=optimizer,\n",
        "                epochs=epochs,\n",
        "                device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhcq1PGywZCw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming StateFarmDataset and SparseFilter are already defined\n",
        "# Assuming config and target_transform are already set\n",
        "\n",
        "# Load the dataset without transformations\n",
        "#train_data = StateFarmDataset(config,\n",
        " #                             transform=None,  # No image transformations\n",
        " #                             split='train',\n",
        " #                             target_transform=None,\n",
        " #                             apply_sparse_filtering=False)\n",
        "\n",
        "\n",
        "# Prepare data for sparse filtering (flatten the data)\n",
        "# Note: Ensure that your dataset __getitem__ method returns the raw data without transformations\n",
        "#flattened_data = []\n",
        "#for data, _ in train_loader:\n",
        "#    data = data.float()  # Convert data to float\n",
        "#    data_flattened = data.reshape(-1).to(device)  # Move data to GPU\n",
        "#    flattened_data.append(data_flattened)\n",
        "\n",
        "input_dim = 168*224 #flattened_data.shape[1] # [15646, 921600]\n",
        "output_dim = 168*224  # Set the desired output dimension\n",
        "##sparse_filter_model = train_sparse_filter(train_loader, input_dim, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBeeYrCOwZCw"
      },
      "outputs": [],
      "source": [
        "#optimizer = optim.Adam(sparse_filter_model.parameters(), lr=0.01, weight_decay=1e-5)  # L2 regularization\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "#for epoch in range(50):\n",
        "#    for data, _ in train_loader:\n",
        "#        data = data.float()  # Convert data to float\n",
        "#        data_flattened = data.reshape(-1).to(device)  # Move data to GPU\n",
        "#\n",
        "#        optimizer.zero_grad()\n",
        "#        loss = sparse_filter_model(data_flattened)  # Forward pass on the batch\n",
        "#        loss.backward()  # Compute gradients\n",
        "#        optimizer.step()  # Update weights\n",
        "#    scheduler.step()\n",
        "\n",
        "#    print(f'Epoch {epoch}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT_ggsbfwZCw"
      },
      "outputs": [],
      "source": [
        "# Apply Sparse Filtering Transformation to Data\n",
        "#with torch.no_grad():\n",
        "#transformed_X = torch.matmul(X_flattened, model.weights).detach()\n",
        "\n",
        "# Now you can use transformed_X as input to your TinyVGGish model or other models\n",
        "\n",
        "# Note: Adjust input_dim, output_dim, and num_epochs according to your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqpxHyB2wZCw"
      },
      "outputs": [],
      "source": [
        "# Save the model weights\n",
        "torch.save(sparse_filter_model.state_dict(), './outputs/SparseFilterWeights/sparse_filter_model_try2.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TCpNU2IoMd6"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.save(sparse_filter_model.state_dict(), './drive/MyDrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDaRwhJjKzFY"
      },
      "source": [
        "## `TinyVGG`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T9mX5BKKzFY"
      },
      "outputs": [],
      "source": [
        "# IMG Transformations\n",
        "augmentations = {\n",
        "    'train': v2.Compose([\n",
        "    # v2.RandomRotation(degrees=30),\n",
        "    v2.RandomResizedCrop((168, 224), antialias=True, scale=(0.9, 1)),\n",
        "    #v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.ToDtype(torch.float32, scale=True)]),\n",
        "    'val+test': v2.Compose([\n",
        "    T.Resize((168, 224), antialias=True),\n",
        "    v2.ToDtype(torch.float32, scale=True)])}\n",
        "\n",
        "# Target Transformations (Removing the c from the target)\n",
        "target_transform = T.Lambda(lambda y: torch.tensor(int(y.replace('c', ''))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXckhcO8KzFX"
      },
      "outputs": [],
      "source": [
        "# Creating the dataset\n",
        "train_data = StateFarmDataset(config,\n",
        "                              transform=augmentations['train'],\n",
        "                              split='train',\n",
        "                              target_transform=target_transform,\n",
        "                              apply_sparse_filtering=True)\n",
        "\n",
        "print(f'Lenght of train data: {len(train_data)}')\n",
        "\n",
        "# Creating the dataset\n",
        "val_data = StateFarmDataset(config,\n",
        "                            transform=augmentations['val+test'],\n",
        "                            split='val',\n",
        "                            target_transform=target_transform,\n",
        "                            apply_sparse_filtering=True)\n",
        "\n",
        "print(f'Lenght of val data: {len(val_data)}')\n",
        "\n",
        "test_data = StateFarmDataset(config,\n",
        "                            split='test',\n",
        "                            transform=augmentations['val+test'],\n",
        "                            target_transform=target_transform,\n",
        "                            apply_sparse_filtering=True)\n",
        "\n",
        "print(f'Lenght of val data: {len(test_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOlK-farQYyk"
      },
      "source": [
        "#### `Model`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUxzvRBVQYyk"
      },
      "outputs": [],
      "source": [
        "# TinyVGG inpsired\n",
        "# First Convolution Blocks With BatchNorm, MaxPool and Dropout\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channesl, out_channls, kernel_size = (3, 3), stride=1, pool_kernel = (2,2), dropout_rate = .2):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channesl,\n",
        "                              out_channels=out_channls,\n",
        "                              kernel_size=kernel_size,\n",
        "                              stride=stride),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(out_channls),\n",
        "            nn.Conv2d(in_channels=out_channls,\n",
        "                              out_channels=out_channls,\n",
        "                              kernel_size=kernel_size,\n",
        "                              stride=stride),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm2d(out_channls),\n",
        "            nn.MaxPool2d(kernel_size = pool_kernel),\n",
        "            nn.Dropout(dropout_rate))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "# Added couple extra fully connected layers\n",
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, in_features, out_featuers, dropout_rate) -> None:\n",
        "        super(FullyConnected, self).__init__()\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(in_features=in_features, out_features=out_featuers),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(out_featuers),\n",
        "            nn.Dropout(dropout_rate),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "\n",
        "class TinyVGGish(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 filters = 32,\n",
        "                 num_classes = 10,\n",
        "                 kernel_size = 3,\n",
        "                 stride = 1,\n",
        "                 in_channels = 3,\n",
        "                 pool_kernel_size = 2,\n",
        "                 dense_dropout_rate = .25,\n",
        "                 conv_dropout_rate = .25):\n",
        "\n",
        "        \"\"\"TinyVGG Inspired Model with Added complexity and Regularizaiton\"\"\"\n",
        "        super(TinyVGGish, self).__init__()\n",
        "\n",
        "        # First Convolution Block\n",
        "        self.main = nn.Sequential(\n",
        "            ConvBlock(in_channesl=in_channels, out_channls=filters, kernel_size=kernel_size, stride=stride, pool_kernel=pool_kernel_size, dropout_rate=conv_dropout_rate),\n",
        "            ConvBlock(in_channesl=filters, out_channls=filters*2, kernel_size=kernel_size, stride=stride, pool_kernel=pool_kernel_size, dropout_rate=conv_dropout_rate),\n",
        "            ConvBlock(in_channesl=filters*2, out_channls=filters*4, kernel_size=kernel_size, stride=stride, pool_kernel=pool_kernel_size, dropout_rate=conv_dropout_rate),\n",
        "            nn.Flatten(),\n",
        "            FullyConnected(in_features=52224, out_featuers=512, dropout_rate=dense_dropout_rate),\n",
        "            FullyConnected(in_features=512, out_featuers=128, dropout_rate=dense_dropout_rate),\n",
        "            FullyConnected(in_features=128, out_featuers=num_classes, dropout_rate=dense_dropout_rate),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the data through the convolutional blocks\n",
        "        x = self.main(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvw9GCTCQYyk"
      },
      "outputs": [],
      "source": [
        "# Initialize Efficientnet model\n",
        "model = TinyVGGish(num_classes = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZbi4fJNQYyk"
      },
      "outputs": [],
      "source": [
        "batch_size = config['modeling_params']['batch_size']\n",
        "epochs = 50 # config['modeling_params']['epochs']\n",
        "seed = 42\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-YbQ3UQwZCy"
      },
      "outputs": [],
      "source": [
        "print(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXvq116rQYyk"
      },
      "outputs": [],
      "source": [
        "# Model summary\n",
        "x, y = next(iter(train_dataloader))\n",
        "\n",
        "summary(model, input_size=x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qijKuDacQYyl"
      },
      "source": [
        "#### `Training Methods`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUs_W8bxwZCz"
      },
      "outputs": [],
      "source": [
        "def visualize_training(history, num_epochs=50):\n",
        "\n",
        "    # Generate Figure\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Loss Plots\n",
        "    sns.lineplot(y=history['train_loss'], x=list(range(len(history['train_loss']))), ax=axs[0], label='Train Loss')\n",
        "    sns.lineplot(y=history['val_loss'], x=list(range(len(history['val_loss']))), ax=axs[0], label='Validation Loss')\n",
        "    axs[0].set_ylabel('Cross Entropy Loss')\n",
        "    axs[0].set_xlabel('Epochs')\n",
        "    axs[0].set_xlim(0, num_epochs)\n",
        "\n",
        "    # Accuracy Plots\n",
        "    sns.lineplot(y=history['train_acc'], x=list(range(len(history['train_acc']))), ax=axs[1], label='Train Accuracy')\n",
        "    sns.lineplot(y=history['val_acc'], x=list(range(len(history['val_acc']))), ax=axs[1], label='Validation Accuracy')\n",
        "    axs[1].set_ylabel('Accuracy')\n",
        "    axs[1].set_xlabel('Epochs')\n",
        "    axs[1].set_xlim(0, num_epochs)\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPo-51GAQYyl"
      },
      "outputs": [],
      "source": [
        "def train_step(model, dataloader, loss_fn,optimizer, device):\n",
        "    \"\"\"Train step for a single epoch. Taken from PyTorch 'Training with PyTorch'\"\"\"\n",
        "\n",
        "    # Losses and accuracies\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "\n",
        "        # Extracting data and labels + moving to device\n",
        "        imgs, labels = data\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero-ing gradients for every new batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        prediction = model(imgs)\n",
        "\n",
        "        # Computing Loss and Gradient\n",
        "        loss = loss_fn(prediction, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update train loss and accuracy\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (prediction.argmax(1) == labels).type(torch.float).mean().item()\n",
        "    # Return train loss and accuracy\n",
        "    return train_loss / len(dataloader), train_acc / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P13K9bx-QYyl"
      },
      "outputs": [],
      "source": [
        "def validation(model, dataloader, loss_fn, device):\n",
        "    \"\"\"Validation loop\"\"\"\n",
        "    # Setup validation loss and accuracy\n",
        "    val_loss, val_acc = 0, 0\n",
        "\n",
        "    # Disable gradient calculations\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):\n",
        "            # Extract imgs and labels and sent to device\n",
        "            imgs, labels = data\n",
        "            imgs, labels  = imgs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass and update validation loss\n",
        "            prediction = model(imgs)\n",
        "            loss = loss_fn(prediction, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Update validation accuracy\n",
        "            val_acc += (prediction.argmax(1) == labels).type(torch.float).mean().item()\n",
        "    # Return validation loss and accuracy\n",
        "    return val_loss / len(dataloader), val_acc / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMyB8T-lwZCz"
      },
      "outputs": [],
      "source": [
        "def save_model(model, model_name, epoch):\n",
        "    \"\"\"Function for saving model\"\"\"\n",
        "    # Model name, with path\n",
        "    timestamp = dt.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    file = f'{model_name}_{timestamp}_epoch_{epoch+1}'\n",
        "    name = os.path.join(config['outputs']['path'], model_name, file)\n",
        "\n",
        "    # Make directory if not exists\n",
        "    if not os.path.exists(os.path.join(os.path.join(config['outputs']['path'], model_name))):\n",
        "        os.makedirs(os.path.join(os.path.join(config['outputs']['path'], model_name)))\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), f'{name}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7omRMNMwZCz"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "    \"\"\"Early Stopping Class. Copied from https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\"\"\"\n",
        "    def __init__(self, patience=1, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = float('inf')\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            if self.counter != 0:\n",
        "                print('Early Stopping Counter Reset')\n",
        "            self.counter = 0\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            print(f'Early Stopping Counter {self.counter} of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-ndlrjVQYyl"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataloader, validation_dataloader, optimizer, loss_fn, epochs, device, scheduler, early_stopping):\n",
        "    \"\"\"Model training method\"\"\"\n",
        "    # History\n",
        "    history = dict(train_loss=[],\n",
        "                   train_acc=[],\n",
        "                   val_loss=[],\n",
        "                   val_acc=[])\n",
        "\n",
        "    # Loop through epochs\n",
        "    for epoch in range(epochs):\n",
        "        print(f'\\nEpoch {epoch+1} of {epochs} started...')\n",
        "\n",
        "        # Set model to train mode and do pass over data\n",
        "        model.train(True)\n",
        "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
        "\n",
        "        # Set model to eval and do pass over validation data\n",
        "        model.eval()\n",
        "        val_loss, val_acc = validation(model, validation_dataloader, loss_fn, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} of {epochs} - Train loss: {train_loss:.5f} - Train acc: {train_acc:.5f} - Val loss: {val_loss:.5f} - Val acc: {val_acc:.5f}\")\n",
        "\n",
        "        # lr Scheduler step\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "        # Print info\n",
        "\n",
        "\n",
        "        # Save model if val loss is lower than previous lowest\n",
        "        if val_loss < min(history['val_loss'], default=1e10):\n",
        "            print(f\"Saving model with new best val_loss: {val_loss:.5f}\")\n",
        "\n",
        "            # Save model\n",
        "            save_model(model, MODEL_NAME, epoch)\n",
        "\n",
        "        # Save train and val loss/acc\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Visualize every 5th epoch\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            visualize_training(history, epochs)\n",
        "\n",
        "        if early_stopping.early_stop(val_loss):\n",
        "            print(f\"Epoch {epoch+1} of {epochs} - Early stopping\")\n",
        "            print('Saving final model, with loss: ', val_loss)\n",
        "            save_model(model, MODEL_NAME, epoch)\n",
        "            visualize_training(history, epoch+1)\n",
        "            break\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXlbbfALwZC0"
      },
      "outputs": [],
      "source": [
        "# Setting seed and general training setup\n",
        "torch.manual_seed(seed)\n",
        "epcohs = 50\n",
        "learming_rate = 0.01\n",
        "momentum = .9\n",
        "weight_decay = 1e-5\n",
        "# nestrov = True\n",
        "\n",
        "# Creating Model Object\n",
        "model = TinyVGGish(num_classes = 10, dense_dropout_rate=.5).to(device)\n",
        "\n",
        "# Optimizer and and scheduler\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learming_rate, weight_decay=weight_decay, momentum=momentum, nesterov=nestrov)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=learming_rate, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                       T_max = epochs,\n",
        "                                                       eta_min = 0,\n",
        "                                                       verbose=1)\n",
        "\n",
        "# Visualizing scheduler effect on learning rate\n",
        "lrs = []\n",
        "for i in range(100):\n",
        "    scheduler.step()\n",
        "    lrs.append(scheduler.get_last_lr()[0])\n",
        "\n",
        "plt.plot(lrs)\n",
        "\n",
        "# Loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopper(patience=7, min_delta=.03)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiA2xTYdwZC0"
      },
      "source": [
        "##### `Training`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVasakcHQYyl"
      },
      "outputs": [],
      "source": [
        "# results\n",
        "results = train(model=model,\n",
        "                train_dataloader=train_dataloader,\n",
        "                validation_dataloader=val_dataloader,\n",
        "                optimizer=optimizer,\n",
        "                loss_fn=loss_fn,\n",
        "                epochs=epochs,\n",
        "                device=device,\n",
        "                scheduler=scheduler,\n",
        "                early_stopping=early_stopping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fr8z19_wZC0"
      },
      "source": [
        "### Testing on Test Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4tFaaOUwZC0"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import read_image\n",
        "\n",
        "test_imgs = os.listdir(config['dataset']['images']['test'])\n",
        "test_img = test_imgs[random.randint(0, len(test_imgs))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV4LqiIUwZC1"
      },
      "outputs": [],
      "source": [
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5Qg5GE2wZC1"
      },
      "outputs": [],
      "source": [
        "for i, data in enumerate(test_dataloader):\n",
        "    imgs, labels = data\n",
        "    imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    prediction = model(imgs)\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsxUMh7gwZC1"
      },
      "outputs": [],
      "source": [
        "prediction.argmax(1)[0].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF6M-jkEwZC2"
      },
      "outputs": [],
      "source": [
        "index = 16\n",
        "print('Prediction:', config['dataset']['class_mapping']['c' + str(prediction.argmax(1)[index].item())])\n",
        "print('True:', config['dataset']['class_mapping']['c' + str(labels[index].item())])\n",
        "\n",
        "plt.imshow(imgs[index].cpu().permute(1, 2, 0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "3c8ad80dbc202ee9aff426fa151d52828bd7b4f3aec4c0ec93048b40e8792c84"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}